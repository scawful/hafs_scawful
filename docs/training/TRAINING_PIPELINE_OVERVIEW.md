# Complete Training Pipeline Overview

## Current Status (2025-12-21)

### Running Campaigns

**Campaign 1: gemini-3-flash-preview (Fast Generation)**
- Progress: 15/16 ASM samples (94%)
- Target: 100 samples total
- Model: gemini-3-flash-preview
- Status: Completing first domain

**Campaign 2: gemini-3-pro-preview (High Quality)**
- Progress: 4/33 ASM samples (12%)
- Target: 200 samples total
- Model: gemini-3-pro-preview (better reasoning/coding)
- Status: Just started

**Model Installation (medical-mechanica)**
- âœ“ qwen2.5-coder: 14B, 7B, 32B
- âœ“ deepseek-coder: 6.7B, 33B
- âœ“ gemma2: 9B
- âœ“ llama3.2: 3B
- âœ“ mistral: 7B
- âœ“ phi3.5:latest
- Total: ~150GB of models installed

## Complete Training Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    1. DATA GENERATION                        â”‚
â”‚  (Currently Running - 2 parallel campaigns)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              2. SUPERVISED FINE-TUNING (LoRA)                â”‚
â”‚  Train parameter-efficient adapters on your GPU              â”‚
â”‚  â€¢ Input: Training JSONL (~10K samples)                      â”‚
â”‚  â€¢ Output: LoRA adapters (~4M params)                        â”‚
â”‚  â€¢ Time: 4-8 hours on RTX 4070 Ti SUPER                      â”‚
â”‚  â€¢ VRAM: 12-16GB                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   3. EVALUATION                              â”‚
â”‚  Benchmark model on test sets                                â”‚
â”‚  â€¢ Automatic metrics (perplexity, BLEU)                      â”‚
â”‚  â€¢ Domain benchmarks (ASM, ROM hack)                         â”‚
â”‚  â€¢ LLM-as-judge (Claude Opus)                                â”‚
â”‚  â€¢ Human evaluation (optional)                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            4. PREFERENCE OPTIMIZATION (DPO)                  â”‚
â”‚  Improve model with human preferences                        â”‚
â”‚  â€¢ Input: Preference pairs (chosen vs rejected)              â”‚
â”‚  â€¢ Output: DPO-tuned model                                   â”‚
â”‚  â€¢ Time: 2-4 hours                                           â”‚
â”‚  â€¢ Improvement: +5-10% on preference metrics                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   5. DEPLOYMENT                              â”‚
â”‚  Export to Ollama or vLLM for serving                        â”‚
â”‚  â€¢ Option A: Ollama (easiest)                                â”‚
â”‚  â€¢ Option B: vLLM (fastest)                                  â”‚
â”‚  â€¢ Option C: HuggingFace (flexible)                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Step-by-Step Guide

### Step 1: Data Generation âœ… IN PROGRESS

```bash
# Already running two campaigns:
# 1. Fast generation with gemini-3-flash (100 samples)
# 2. High-quality with gemini-3-pro (200 samples)

# Monitor progress:
tail -f ~/.context/logs/campaign_*.log

# When complete, you'll have:
# - ~/.context/training/pilot_hybrid_100_*/samples.jsonl
# - ~/.context/training/pro_coding_200/samples.jsonl
```

**ETA:** Campaign 1 finishes in ~10 min, Campaign 2 in ~4 hours

### Step 2: LoRA Training (When data ready)

```bash
# Transfer data to Windows GPU server
scp ~/.context/training/*/samples.jsonl medical-mechanica:D:/hafs_training/datasets/

# SSH to medical-mechanica
ssh medical-mechanica

# Run LoRA training
cd D:/hafs_training
python -m agents.training.lora_trainer \
  --base_model Qwen/Qwen2.5-Coder-14B-Instruct \
  --dataset D:/hafs_training/datasets/samples.jsonl \
  --output_dir D:/hafs_training/checkpoints/hafs-coder-v1 \
  --lora_r 64 \
  --lora_alpha 128 \
  --batch_size 4 \
  --gradient_accumulation_steps 4 \
  --learning_rate 2e-4 \
  --num_epochs 3 \
  --fp16 \
  --gradient_checkpointing
```

**ETA:** 4-8 hours
**Output:** LoRA adapters at `D:/hafs_training/checkpoints/hafs-coder-v1/final`

### Step 3: Evaluation

```bash
# Automatic evaluation
python -m agents.training.eval.benchmark \
  --model hafs-coder:14b \
  --benchmarks asm rom_hack code_understanding \
  --output ~/.context/training/eval_results_$(date +%Y%m%d).json

# Compare to baseline
python -m agents.training.eval.compare \
  --baseline Qwen/Qwen2.5-Coder-14B-Instruct \
  --finetuned hafs-coder:14b \
  --report ~/.context/training/comparison_report.md
```

**Expected Results:**
- ASM Benchmark: 48% â†’ 75% (+27%)
- ROM Hack: 40% â†’ 75% (+35%)
- Code Quality: +2.0 points (1-10 scale)

### Step 4: DPO (Optional - for further improvement)

```bash
# Generate preference pairs
python -m agents.training.generate_preferences \
  --model hafs-coder:14b \
  --num-samples 5000 \
  --candidates-per-prompt 4 \
  --scorer llm \
  --output ~/.context/training/preferences.jsonl

# Run DPO training
python -m agents.training.dpo_trainer \
  --sft_model hafs-coder:14b \
  --preference_data ~/.context/training/preferences.jsonl \
  --output_dir D:/hafs_training/checkpoints/hafs-coder-dpo \
  --beta 0.1 \
  --learning_rate 5e-6 \
  --num_epochs 1
```

**ETA:** 2-4 hours
**Expected Improvement:** 75% â†’ 82% on benchmarks

### Step 5: Deployment

```bash
# Option A: Export to Ollama (recommended)
ollama create hafs-coder:14b -f Modelfile

# Option B: Serve with vLLM (fastest)
python -m vllm.entrypoints.openai.api_server \
  --model D:/hafs_training/checkpoints/hafs-coder-v1/final \
  --host 0.0.0.0 --port 8000

# Option C: Load in Python
from transformers import AutoModelForCausalLM
model = AutoModelForCausalLM.from_pretrained("hafs-coder-v1/final")
```

## Infrastructure Created

### Documentation
- âœ… `docs/training/LORA_TRAINING.md` - Complete LoRA guide
- âœ… `docs/training/EVALUATION.md` - Evaluation framework
- âœ… `docs/training/RLHF_DPO.md` - Preference optimization
- âœ… `docs/training/TRAINING_PIPELINE_OVERVIEW.md` - This file

### Implementation
- âœ… `src/agents/training/lora_trainer.py` - LoRA training script
- âœ… `src/agents/training/hybrid_campaign.py` - Data generation (updated)
- ğŸ”² `src/agents/training/dpo_trainer.py` - DPO training (TODO)
- ğŸ”² `src/agents/training/eval/` - Evaluation scripts (TODO)

### Configuration
- âœ… `config/training_medical_mechanica.toml` - GPU server template (copy into your plugin)
- âœ… Hybrid thresholds tuned (30-60% GPU usage)
- âœ… Model timeout increased to 120s
- âœ… Multi-model support (gemini-3-flash, gemini-3-pro)

## Resource Requirements

### Data Generation (Mac/API)
- **Compute:** Gemini API (paid)
- **Cost:** ~$5-20 per 10K samples
- **Time:** 2-4 hours for 10K samples
- **Storage:** ~50MB per 10K samples

### LoRA Training (Windows GPU)
- **GPU:** RTX 4070 Ti SUPER (16GB VRAM) âœ…
- **VRAM:** 12-16GB for 14B model
- **Time:** 4-8 hours
- **Storage:** ~20GB (model + checkpoints)

### DPO Training (Windows GPU)
- **GPU:** Same as LoRA
- **VRAM:** ~14GB
- **Time:** 2-4 hours
- **Storage:** +5GB (DPO checkpoints)

### Inference (Either)
- **Ollama:** 9GB VRAM (14B model)
- **vLLM:** 12GB VRAM (faster)
- **CPU:** Possible but slow (~1 tok/s)

## Next Actions

### Immediate (Today)
1. âœ… Wait for campaigns to complete (~10 min for Campaign 1, ~4 hours for Campaign 2)
2. Verify generated data quality
3. Combine datasets if desired

### Short Term (This Week)
1. Transfer data to medical-mechanica
2. Run LoRA training (4-8 hours)
3. Evaluate fine-tuned model
4. Test with hafs CLI integration

### Medium Term (Next Week)
1. Generate preference pairs
2. Run DPO training
3. Final evaluation
4. Deploy to Ollama
5. Document results

### Long Term
1. Collect production feedback
2. Iterative improvement (DPO round 2)
3. Larger dataset (34K samples)
4. Multi-task training (add more domains)

## Key Advantages of Your Setup

1. **Hybrid GPU+API:** Best of both worlds (free GPU + fast API)
2. **LoRA:** Train 14B models on your 16GB GPU
3. **DPO:** Improve without complex RLHF setup
4. **Multi-model:** Compare gemini-flash vs pro quality
5. **Automated:** End-to-end pipeline from data â†’ deployment

## Questions?

- **LoRA details:** See `docs/training/LORA_TRAINING.md`
- **How to evaluate:** See `docs/training/EVALUATION.md`
- **DPO/RLHF:** See `docs/training/RLHF_DPO.md`
- **GPU setup:** See `docs/training/GPU_ACCELERATION.md`

## Support

If you run into issues:
1. Check logs in `~/.context/training/logs/`
2. GPU memory issues â†’ reduce batch_size or lora_rank
3. Quality issues â†’ generate more data or adjust thresholds
4. Training divergence â†’ lower learning_rate

Happy training! ğŸš€
