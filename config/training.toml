# Training Configuration for medical-mechanica (GPU Server)
# Use D drive for large datasets and model storage

[paths]
# D drive paths (Windows)
datasets = "D:/hafs_training/datasets"
checkpoints = "D:/hafs_training/checkpoints"
logs = "D:/hafs_training/logs"
models = "D:/hafs_training/models"
temp = "D:/hafs_training/temp"

# Source code and knowledge bases (C drive is fine)
knowledge_bases = "C:/Users/Administrator/.context/knowledge"
embeddings = "C:/Users/Administrator/.context/embeddings"

[campaign]
# Default campaign settings
target_samples = 34500
quality_threshold = 0.7  # Overridden by per-domain thresholds (use None for auto)
checkpoint_interval = 100  # Save every N samples
parallel_workers = 100  # Increased for maximum throughput (Gemini API: 1000 RPM)
max_concurrent_generations = 100  # Parallel generation streams

# Domain-specific thresholds (validated 2025-12-21)
[campaign.thresholds]
asm = 0.4
gigaleak = 0.45
oracle = 0.4
yaze = 0.5
cpp = 0.5
errors = 0.3
text = 0.6

[training]
# Model training settings (when training on medical-mechanica)
base_model = "Qwen/Qwen2.5-Coder-14B-Instruct"
use_lora = true
lora_r = 64
lora_alpha = 128
batch_size = 4
gradient_accumulation_steps = 4
learning_rate = 2e-4
num_epochs = 3
warmup_steps = 100

# Hardware settings
device = "cuda"
fp16 = true
gradient_checkpointing = true

[monitoring]
# Health check and cleanup
auto_checkpoint_cleanup = true
max_checkpoint_age_days = 7
disk_alert_threshold_gb = 50
memory_alert_threshold_percent = 85

[api]
# API rate limits for generation
gemini_rpm = 1000  # Requests per minute
gemini_tpm = 4000000  # Tokens per minute
max_retries = 3
retry_delay_seconds = 5

[gpu]
# GPU acceleration settings (medical-mechanica)
enabled = true
use_local_models = true  # Use Ollama on GPU instead of Gemini
ollama_host = "100.104.53.21"  # Tailscale IP
ollama_port = 11434  # Standard Ollama port

# Available models (after installation completes)
models = [
    "qwen2.5-coder:14b",    # Best for C++/Python code
    "deepseek-coder:6.7b",  # Best for ASM
    "qwen2.5-coder:7b",     # Fast alternative
    "phi3.5:latest",        # Very fast
    "gemma2:9b",            # Latest Gemma
    "codegemma:7b",         # Code-specialized Gemma
    "qwen3-14b",            # Fallback (already installed)
    "deepseek-r1:14b",      # Reasoning
]

# Domain-specific model routing for optimal quality
[gpu.routing]
asm = "deepseek-coder:6.7b"        # ASM needs low-level specialist
cpp = "qwen2.5-coder:14b"          # C++ general code
yaze = "qwen2.5-coder:14b"         # C++ emulator code
gigaleak = "deepseek-coder:6.7b"   # Original SNES source
oracle = "qwen2.5-coder:14b"       # ROM hack mix
text = "gemma2:9b"                 # Natural language
errors = "deepseek-r1:14b"         # Error reasoning

# Fallback chain if primary model unavailable
fallback = ["qwen3-14b", "phi3.5:latest", "gemma2:9b"]

# Fallback to Gemini if GPU unavailable
fallback_to_gemini = true

[hybrid]
# Hybrid GPU + API mode (RECOMMENDED)
# Intelligently routes between GPU (free) and API (paid) based on GPU load
enabled = true
gpu_threshold_low = 30.0   # Below this %: prefer GPU (lowered to use more API)
gpu_threshold_high = 60.0  # Above this %: prefer API (lowered for faster response)
monitor_interval_sec = 5   # GPU status cache TTL

# Expected cost savings: 30-50% vs API-only (balanced for speed)
# GPU utilization thresholds:
#   <30%: Route to GPU (FREE) - use when idle
#   30-60%: Mix GPU + API based on load - prefer API for speed
#   >60%: Route to API (faster response, prevent overload)
