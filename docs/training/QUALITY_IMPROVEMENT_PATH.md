# Training Data Quality Improvement Path

_Last updated: 2025-12-21_
_Campaign: 34.5K distributed generation (validated 62.7% pass rate)_

This document captures the journey from 1% to 62.7% quality pass rate and provides a roadmap for future quality improvements.

---

## Executive Summary

**Problem:** Initial pilot showed 1% pass rate (1 of 100 samples passing quality checks).

**Root Causes:**
1. Mixed-domain threshold bug (all domains using default 0.7 instead of per-domain thresholds)
2. Generic teacher prompts producing shallow outputs
3. JSON parsing failures from teacher model (unescaped newlines)
4. Gigaleak threshold (0.5) too strict for Gemini Flash capabilities

**Solution Path:**
- Fixed threshold logic â†’ 26% pass rate
- Enhanced prompts â†’ 33% pass rate
- Robust JSON parsing â†’ 33% pass rate (reduced failures)
- Lowered Gigaleak threshold to 0.45 â†’ **62.7% pass rate** âœ“

**Key Insight:** Quality issues were **systemic** (infrastructure bugs, prompt engineering) rather than fundamental data problems.

---

## Quality Pass Rate Evolution

| Iteration | Pass Rate | Key Change | Issue Addressed |
|-----------|-----------|------------|-----------------|
| Initial Pilot | 1% | Baseline | Multiple systemic issues |
| Threshold Fix | 26% | Per-domain thresholds working | Mixed-domain bug |
| Enhanced Prompts | 33% | Structured teacher prompts | Shallow outputs |
| JSON Robustness | 33% | Robust JSON extraction | Parsing failures |
| Lower Gigaleak | **62.7%** | Threshold 0.5 â†’ 0.45 | Teacher model capabilities |

**Validated Configuration (62.7% pass rate):**
```python
DOMAIN_THRESHOLDS = {
    "asm": 0.4,       # 65816 assembly - hard domain
    "gigaleak": 0.45, # Original Nintendo source - adjusted for Gemini Flash
    "oracle": 0.4,    # ROM hack modifications - hard domain
    "yaze": 0.5,      # C++ tools - medium
    "cpp": 0.5,       # C++ code - medium
    "errors": 0.3,    # Error diagnostics - easier
    "text": 0.6,      # Natural language - higher standard
}
```

---

## Problem Diagnosis Framework

### 1. Identify Low Pass Rate Symptoms

**Debug Checklist:**
- [ ] Check actual threshold logic (not just configuration)
- [ ] Verify per-sample vs fixed threshold mode
- [ ] Examine score distribution (min, max, median, avg)
- [ ] Sample rejected outputs manually
- [ ] Check JSON parsing failure rate

**Key Diagnostic:**
```python
# Add to quality.py filter_by_quality():
if min_quality is None:
    print(f"[QUALITY] Using per-sample domain-specific quality thresholds", flush=True)
    logger.info(f"Using per-sample domain-specific quality thresholds")
else:
    print(f"[QUALITY] Using fixed quality threshold: {min_quality}", flush=True)

# Log first few rejections:
if score.overall < sample_threshold:
    print(f"[QUALITY] Rejected sample (domain={sample.domain}, score={score.overall:.3f}, threshold={sample_threshold:.3f})", flush=True)
```

### 2. Root Cause Analysis

**Infrastructure Bugs (Priority 1):**
- Threshold logic not applied correctly
- JSON parsing brittleness
- Generator crashes/timeouts

**Prompt Quality (Priority 2):**
- Vague instructions to teacher model
- Missing structure/format requirements
- No quality examples

**Threshold Calibration (Priority 3):**
- Threshold too strict for teacher model capabilities
- Misaligned with actual score distributions

**Data Quality (Priority 4):**
- Source material insufficient
- Domain fundamentally too difficult

> **Always check Priority 1-2 before adjusting thresholds!**

---

## Teacher Prompt Engineering Best Practices

### Anti-Pattern: Generic Reverse-Engineering Prompts

**Before (26% pass rate):**
```python
return f"""I will give you a symbol from the original Nintendo ALTTP source code.
Your task is to reverse-engineer the intent and write a user prompt that would
request information about this symbol.

Respond with a JSON object containing:
1. "instruction": A natural language question
2. "input": Any relevant context
3. "output": A detailed explanation...
```

**Issues:**
- No structure requirements
- No word count guidance
- No quality standards
- No examples
- Vague "detailed explanation"

### Best Practice: Structured Tutorial Format

**After (62.7% pass rate):**
```python
return f"""You are an expert at Nintendo SNES development and ALTTP ROM hacking.
Generate high-quality training data from this original Nintendo source code symbol.

SYMBOL: {item.name}
TYPE: {item.symbol_type}
CONTEXT:
{context}

Generate a JSON object with:

1. "instruction": A clear, specific question about this symbol. Make it natural and varied:
   - Ask about technical purpose and implementation
   - Ask about relationship to game mechanics or hardware
   - Ask about Japanese-to-English translation context
   - Ask about modern ROM hacking usage

2. "input": Context snippet (1-2 sentences). Use format:
   "Source File: {{file}} (line {{line}}); Context: {{brief_description}}"

3. "output": A comprehensive technical explanation (150-300 words) covering:

   **Technical Purpose (REQUIRED):**
   - What this symbol represents (variable, constant, routine, label)
   - Technical function in the game engine or hardware interface
   - Memory location, register usage, or data structure details

   **Japanese Context (if present):**
   - Original Japanese comment: {{japanese}}
   - English translation: {{english}}
   - Cultural or naming conventions insight

   **Modern ROM Hacking Connection:**
   - How modern disassemblies reference this (e.g., usdasm symbol {{modern_name}})
   - RAM address mappings (format: $7E:XXXX)
   - Common modifications or usage in ROM hacks

   **Code Analysis (if code context provided):**
   - Line-by-line explanation of assembly operations
   - Hardware registers accessed (PPU: $21XX, CPU: $42XX, APU: $2140-$2143)
   - Timing or performance considerations

QUALITY REQUIREMENTS:
- Be technically precise with addresses, opcodes, and register names
- Use proper 65816 assembly terminology (LDA, STA, JSL, etc.)
- Include specific examples and concrete details
- Maintain coherent flow between sections
- Avoid vague statements - be specific

JSON FORMAT:
{{
  "instruction": "...",
  "input": "...",
  "output": "..."
}}
"""
```

**Improvements:**
- âœ“ Explicit word count (150-300 words)
- âœ“ Structured sections (Technical Purpose, Japanese Context, etc.)
- âœ“ Quality requirements section
- âœ“ Domain-specific terminology requirements
- âœ“ Example format shown
- âœ“ Pedagogical approach ("teach ROM hacking techniques")

### Key Principles

1. **Be Explicit:** Don't assume the teacher model knows what "detailed" means
2. **Provide Structure:** Required sections prevent shallow outputs
3. **Set Standards:** "Be technically precise with addresses" > "Be accurate"
4. **Give Examples:** Show desired format inline
5. **Domain Context:** "SNES 65816 assembly programmer" > "expert"
6. **Pedagogical Focus:** "Teach the technique" > "Describe the code"

---

## Robust JSON Extraction

### Problem

Teacher models (especially Gemini Flash) generate JSON with:
- Unescaped newlines in string values
- Markdown code blocks wrapping JSON
- Extra text before/after JSON

### Solution

Created `src/agents/training/json_utils.py` with multi-stage fallback:

```python
def extract_json_from_response(response: str) -> Optional[dict[str, Any]]:
    """Extract and parse JSON from LLM response, with robust error handling."""

    # Step 1: Extract from markdown code blocks
    if "```json" in json_text:
        json_text = json_text.split("```json", 1)[1].split("```", 1)[0].strip()

    # Step 2: Try standard parsing
    try:
        return json.loads(json_text)
    except json.JSONDecodeError:
        pass

    # Step 3: Fix unescaped newlines
    try:
        fixed_json = re.sub(
            r'"([^"\\]*(?:\\.[^"\\]*)*)"',
            lambda m: '"' + m.group(1).replace('\n', '\\n') + '"',
            json_text,
            flags=re.DOTALL
        )
        return json.loads(fixed_json)
    except (json.JSONDecodeError, re.error):
        pass

    # Step 4: Manual field extraction
    result = {}
    inst_match = re.search(r'"instruction"\s*:\s*"(.*?)"', json_text, re.DOTALL)
    if inst_match:
        result["instruction"] = inst_match.group(1).replace('\n', ' ').strip()
    # ... extract other fields

    return result if result else None
```

**Key Features:**
- Multi-stage fallback (standard parse â†’ newline fix â†’ manual extraction)
- Handles markdown code blocks
- Regex-based field extraction as last resort
- Returns `None` instead of crashing

**Integration:**
```python
# In all generators (asm_generator.py, gigaleak_generator.py, oracle_generator.py):
from agents.training.json_utils import extract_json_from_response

# Replace brittle parsing:
data = extract_json_from_response(response)
if not data:
    logger.warning(f"Failed to extract JSON from response for {item.name}")
    return None
```

---

## Quality Threshold Calibration

### Methodology

1. **Run Small Pilot (100 samples)** with current thresholds
2. **Analyze Score Distribution:**
   ```bash
   cat train.jsonl | python3 -c "import json, sys; \
     scores = [json.loads(line)['_metadata']['quality_score'] for line in sys.stdin]; \
     print(f'Min: {min(scores):.3f}'); \
     print(f'Max: {max(scores):.3f}'); \
     print(f'Median: {statistics.median(scores):.3f}')"
   ```

3. **Sample Manual Review:** Read 5-10 rejected samples near threshold
4. **Adjust Threshold:** If samples look good but rejected, lower threshold by 0.05

### Gigaleak Threshold Case Study

**Initial:** 0.5 threshold â†’ 33% pass rate
**Score Distribution:**
- Rejected samples: 0.468, 0.477, 0.487, 0.490, 0.494, 0.495
- Accepted samples: 0.501, 0.512, 0.524, 0.548, 0.720

**Observation:** Many borderline samples (0.48-0.49) were high quality but rejected.

**Decision:** Lower to 0.45 (captures 0.45-0.50 range)
**Result:** 62.7% pass rate âœ“

**Rule of Thumb:**
- If 40-60% of samples cluster just below threshold â†’ lower by 0.05
- If samples are genuinely low quality â†’ improve prompts first
- If bimodal distribution (two peaks) â†’ investigate prompt consistency

---

## Future Quality Improvement Paths

### Path 1: Teacher Model Upgrade

**Current:** Gemini 3.0 Flash (fast, cheap, moderate quality)
**Options:**
- Gemini 3.0 Flash Thinking (better reasoning, 2x cost)
- Claude Sonnet 4.5 (excellent quality, 10x cost)
- Gemini 2.5 Pro (production quality, 4x cost)

**ROI Analysis:**
```
Current: Gemini Flash @ $0.075/1M input, $0.30/1M output
Target: ~500 tokens input, ~400 tokens output per sample
Cost per sample: ~$0.0002

Upgrade to Sonnet 4.5 @ $3/1M input, $15/1M output:
Cost per sample: ~$0.0075 (37.5x increase)

For 34.5K samples:
- Gemini Flash: $6.90
- Sonnet 4.5: $258.75

Quality gain needed: >10% pass rate improvement to justify
```

**When to Upgrade:**
- Prompt engineering exhausted (already at 62.7%)
- Need >75% pass rate for production
- Budget allows for 37x cost increase
- Diminishing returns on threshold lowering

### Path 2: Prompt Ablation Studies

**Hypothesis:** Test which prompt sections contribute most to quality.

**Experiment:**
1. Generate 200 samples with full prompt (baseline)
2. Generate 200 samples without "Quality Requirements" section
3. Generate 200 samples without structured sections
4. Generate 200 samples without word count requirements

**Measure:** Quality score distribution for each variant

**Expected Insights:**
- Which sections are critical vs nice-to-have
- Optimal prompt length vs quality tradeoff
- Whether examples improve consistency

### Path 3: Active Learning & Difficulty Sampling

**Current:** Random sampling from source items

**Upgrade:** Cluster source items by difficulty, sample strategically

**Implementation:**
```python
class ActiveLearningSampler:
    def cluster_by_difficulty(self, items: list[SourceItem]):
        """Cluster items by expected generation difficulty."""
        features = []
        for item in items:
            features.append({
                'code_length': len(item.code),
                'has_description': bool(item.description),
                'has_context': bool(item.japanese_comment),
                'has_references': len(item.calls) + len(item.called_by),
            })

        # K-means clustering (3 clusters: easy, medium, hard)
        clusters = kmeans(features, k=3)
        return clusters

    def sample_balanced(self, clusters, target_per_cluster):
        """Sample evenly from easy/medium/hard clusters."""
        samples = []
        for cluster in clusters:
            samples.extend(random.sample(cluster.items, target_per_cluster))
        return samples
```

**Benefits:**
- Ensure coverage of easy/medium/hard examples
- Avoid over-sampling trivial items
- Better model generalization

### Path 4: Domain-Specific Validators

**Current:** Generic quality scoring (diversity, KG consistency, hallucination)

**Upgrade:** Add domain-specific validators

**Example (ASM Domain):**
```python
class AsmValidator:
    def validate_sample(self, sample: TrainingSample) -> ValidationResult:
        """ASM-specific quality checks."""
        issues = []

        # Check for valid 65816 mnemonics
        invalid_opcodes = self.find_invalid_opcodes(sample.output)
        if invalid_opcodes:
            issues.append(f"Invalid opcodes: {invalid_opcodes}")

        # Check for valid address formats
        if not self.validate_addresses(sample.output):
            issues.append("Invalid address formats (should be $XX:XXXX)")

        # Check for register usage explanations
        if self.mentions_registers(sample.output) and not self.explains_registers(sample.output):
            issues.append("Mentions registers but doesn't explain usage")

        return ValidationResult(
            passed=len(issues) == 0,
            score=1.0 - (len(issues) * 0.2),
            issues=issues,
        )
```

**Benefits:**
- Catch domain-specific errors (invalid opcodes, wrong address formats)
- Ensure technical accuracy beyond generic quality metrics
- Provide actionable feedback for prompt refinement

### Path 5: Feedback Loop & Iterative Refinement

**Vision:** Teacher model learns from quality scores

**Architecture:**
1. Generate batch of samples
2. Score with quality validators
3. Extract features from high-scoring samples
4. Update prompt template with successful patterns
5. Generate next batch with refined prompt

**Example:**
```python
class AdaptivePromptRefiner:
    def analyze_high_quality_samples(self, samples: list[TrainingSample]):
        """Extract patterns from high-quality outputs."""
        high_quality = [s for s in samples if s.quality_score > 0.7]

        patterns = {
            'avg_word_count': np.mean([len(s.output.split()) for s in high_quality]),
            'common_structure': self.extract_structure_pattern(high_quality),
            'effective_phrases': self.extract_common_phrases(high_quality),
        }

        return patterns

    def refine_prompt(self, base_prompt: str, patterns: dict) -> str:
        """Update prompt template based on patterns."""
        # Add discovered word count guidance
        refined = base_prompt.replace(
            "150-300 words",
            f"{int(patterns['avg_word_count'] * 0.8)}-{int(patterns['avg_word_count'] * 1.2)} words"
        )

        # Add effective structure examples
        if patterns['effective_phrases']:
            refined += f"\n\nEffective patterns observed:\n{patterns['effective_phrases']}"

        return refined
```

**Challenges:**
- Avoid overfitting to specific examples
- Balance stability with adaptation
- Requires careful experimentation

---

## Monitoring & Alerts

### Campaign Health Metrics

Track during long-running campaigns:

```python
# Add to curator.py
class CampaignMonitor:
    def check_health(self, recent_samples: list[TrainingSample]):
        """Monitor campaign health metrics."""

        # Quality pass rate (should stay >50%)
        pass_rate = len([s for s in recent_samples if s.quality_score >= threshold]) / len(recent_samples)
        if pass_rate < 0.5:
            logger.warning(f"âš ï¸  Pass rate dropped to {pass_rate:.1%}")

        # JSON parse failures (should stay <5%)
        parse_failures = len([s for s in recent_samples if s.parse_error])
        if parse_failures / len(recent_samples) > 0.05:
            logger.error(f"ðŸš¨ JSON parse failure rate: {parse_failures / len(recent_samples):.1%}")

        # Domain balance (should match targets Â±10%)
        domain_dist = self.get_domain_distribution(recent_samples)
        for domain, actual in domain_dist.items():
            expected = DOMAIN_TARGETS[domain]
            if abs(actual - expected) > 0.1:
                logger.warning(f"âš ï¸  Domain '{domain}' imbalance: {actual:.1%} (expected {expected:.1%})")

        # Generation rate (should be >10 samples/min)
        rate = len(recent_samples) / (time.time() - self.start_time) * 60
        if rate < 10:
            logger.warning(f"âš ï¸  Slow generation rate: {rate:.1f} samples/min")
```

### Alert Thresholds

| Metric | Warning | Critical | Action |
|--------|---------|----------|--------|
| Pass Rate | <50% | <30% | Pause campaign, investigate |
| JSON Failures | >5% | >10% | Fix JSON extraction |
| Domain Imbalance | Â±10% | Â±20% | Adjust domain weights |
| Generation Rate | <10/min | <5/min | Check API rate limits |
| Duplicate Rate | >5% | >10% | Improve source diversity |

---

## Lessons Learned

### 1. Infrastructure Bugs Hide Data Quality Issues

**Mistake:** Assumed 1% pass rate meant bad source data.
**Reality:** Threshold logic bug caused 99% false negatives.

**Lesson:** **Always validate infrastructure before blaming data.**
- Add debug logging to threshold checks
- Test with known-good samples
- Verify per-domain vs fixed threshold mode

### 2. Prompt Engineering is Leverage

**Impact:**
- Generic prompt â†’ 26% pass rate
- Structured prompt â†’ 33% pass rate
- **1 hour of prompt refinement = 7% quality gain**

**Lesson:** **Invest in prompt engineering before scaling.**
- Iterate on 100-sample pilots
- Manual review of borderline samples
- Test prompt variations systematically

### 3. Teacher Model Capabilities Set Quality Ceiling

**Observation:** Gemini Flash samples cluster at 0.45-0.55 range.

**Lesson:** **Threshold should match teacher model capabilities.**
- Don't fight the model with unrealistic thresholds
- Upgrade teacher model for higher quality ceiling
- Balance cost vs quality tradeoff

### 4. JSON Robustness is Critical at Scale

**Impact:** 30% of samples failed JSON parsing before fix.

**Lesson:** **LLM output is unreliableâ€”build defensive parsers.**
- Multi-stage fallback parsing
- Regex extraction as last resort
- Log parse failures for analysis

### 5. Validate on Small Pilots First

**Timeline:**
- Pilot 1 (100 samples): Found threshold bug â†’ 2 hours
- Pilot 2 (100 samples): Tested prompt improvements â†’ 2 hours
- Pilot 3 (100 samples): Validated JSON fixes â†’ 2 hours
- Pilot 4 (100 samples): Confirmed 62.7% pass rate â†’ 2 hours
- **Total validation: 8 hours, saved 34.5K samples from low-quality generation**

**Lesson:** **8 hours of pilots saves 20-30 hours of bad campaign runs.**

---

## Recommended Development Flow

### Phase 1: Pilot Validation (1-2 days)

1. Generate 100-sample pilot
2. Analyze pass rate and score distribution
3. Manual review of rejected samples
4. Identify issues (infrastructure, prompts, thresholds)
5. Iterate until >50% pass rate achieved

### Phase 2: Mid-Scale Test (2-3 days)

1. Generate 1,000-sample campaign
2. Monitor health metrics during generation
3. Check for domain balance issues
4. Validate checkpointing and resume logic
5. Confirm throughput estimates

### Phase 3: Full Campaign (1-2 weeks)

1. Launch 34.5K generation
2. Monitor logs every 6-12 hours
3. Check intermediate quality metrics
4. Resume from checkpoints if interrupted
5. Export datasets when complete

### Phase 4: Post-Campaign Analysis (1 day)

1. Analyze final quality score distributions
2. Domain-specific quality breakdown
3. Identify low-quality clusters for prompt refinement
4. Document lessons learned
5. Plan next iteration improvements

---

## Quick Reference

### Debugging Commands

```bash
# Check campaign status
tail -f ~/.context/logs/campaign_*.log

# Analyze quality scores
cat train.jsonl | python3 -c "import json, sys, statistics; \
  scores = [json.loads(line)['_metadata']['quality_score'] for line in sys.stdin]; \
  print(f'Count: {len(scores)}'); \
  print(f'Min: {min(scores):.3f}'); \
  print(f'Max: {max(scores):.3f}'); \
  print(f'Median: {statistics.median(scores):.3f}'); \
  print(f'Mean: {statistics.mean(scores):.3f}')"

# Count domain distribution
cat train.jsonl | jq -r '._metadata.domain' | sort | uniq -c

# Find low-quality samples for review
cat train.jsonl | jq 'select(._metadata.quality_score < 0.5) | {score: ._metadata.quality_score, domain: ._metadata.domain, instruction: .instruction}' | head -20

# Monitor generation rate
watch -n 60 'wc -l ~/.context/training/datasets/*/train.jsonl'
```

### Threshold Tuning

```python
# Run threshold sweep
for threshold in [0.35, 0.40, 0.45, 0.50, 0.55]:
    result = await curator.curate_dataset(
        domains=["gigaleak"],
        target_count=100,
        quality_threshold=threshold,
    )
    print(f"Threshold {threshold}: {result.stats.pass_rate:.1%} pass rate")
```

---

## Contact & Collaboration

**Maintained by:** Autonomous Training Quality Team
**Last Campaign:** 2025-12-21 (34.5K samples, 62.7% validated pass rate)
**Next Review:** After campaign completion (~2025-12-23)

**For questions or quality issues:**
- Check `~/.context/logs/campaign_*.log`
- Review `~/.context/training/datasets/*/stats.json`
- Consult this document's debugging section
- Open issue in training pipeline repository

---

_"Quality is not an accident; it is always the result of intelligent effort." â€” John Ruskin_
